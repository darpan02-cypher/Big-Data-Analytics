{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/darpan02-cypher/Big-Data-Analytics/blob/main/GroqChatbot/7_text_generation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Groq is a company that provides a platform for building and deploying large language model (LLM) applications. They offer a cloud-based service that includes:\n",
        "\n",
        "-High-performance LLMs: Groq has developed its own LLMs that are optimized for low latency and high throughput.\n",
        "\n",
        "-Easy-to-use API: Their API allows developers to easily integrate LLMs into their applications.\n",
        "\n",
        "-Scalable infrastructure: Groq's infrastructure is designed to handle large volumes of requests and can scale to meet your needs.\n",
        "\n",
        "In essence, Groq provides a way to access and use powerful LLMs without having to manage the underlying infrastructure.\n",
        "\n",
        "Here are some key things to know about Groq:\n",
        "\n",
        "Focus on speed: Groq is known for its fast LLMs, which can be crucial for applications that require real-time responses.\n",
        "Developer-friendly: Their API and documentation are designed to make it easy for developers to get started.\n",
        "End-to-end platform: Groq offers a comprehensive platform that includes everything you need to build and deploy LLM applications."
      ],
      "metadata": {
        "id": "qBPoo0VPMpKp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#The code is using the groq library to interact with the Groq API for running large language models (LLMs).\n",
        "\n",
        "Steps:\n",
        "\n",
        "1. Install Libraries: It installs the necessary libraries: python-dotenv for loading environment variables and groq for interacting with the Groq API.\n",
        "\n",
        "2.Set API Key: It sets up your Groq API key, which is essential for authenticating with the service. The key is either loaded from a .env file or directly set in the code.\n",
        "\n",
        "3.Initialize Client: It initializes a Groq client object using your API key, enabling you to make API calls.\n",
        "\n",
        "4.Make API Call: It uses the client.chat.completions.create method to send a request to the Groq API. It provides a user message (\"Explain the importance of low latency LLMs\") and specifies the LLM model to use (e.g., \"llama3-8b-8192\" or \"flan-t5-xl\").\n",
        "\n",
        "5.Print Response: It prints the response generated by the LLM. This response would be an explanation of the importance of low latency LLMs.\n",
        "\n",
        "In essence, the code is using the Groq API to send a query to an LLM and then prints the LLM's response. The specific query in this case is about the importance of low latency in large language models."
      ],
      "metadata": {
        "id": "ryz9qspoNQ9H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install python-dotenv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "vi86SMqICwNP",
        "outputId": "1e489dbb-1b2a-47d0-8b36-26864b9e7c59"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting python-dotenv\n",
            "  Downloading python_dotenv-1.1.0-py3-none-any.whl.metadata (24 kB)\n",
            "Downloading python_dotenv-1.1.0-py3-none-any.whl (20 kB)\n",
            "Installing collected packages: python-dotenv\n",
            "Successfully installed python-dotenv-1.1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "RpHmwEY-CicW",
        "outputId": "cb7eb034-36d3-4fbb-9f11-92a2c8d5a9ba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: groq in /usr/local/lib/python3.11/dist-packages (0.20.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from groq) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from groq) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from groq) (0.28.1)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from groq) (2.10.6)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from groq) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.10 in /usr/local/lib/python3.11/dist-packages (from groq) (4.12.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->groq) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->groq) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->groq) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->groq) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->groq) (2.27.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install groq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "JW12tcowCicX"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from dotenv import load_dotenv\n",
        "from groq import Groq\n",
        "\n",
        "# Load environment variables\n",
        "load_dotenv()\n",
        "\n",
        "import os\n",
        "os.environ[\"GROQ_API_KEY\"] = \"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\"\n",
        "\n",
        "#client = Groq(\n",
        "   # api_key=os.getenv(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),\n",
        "#)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from dotenv import load_dotenv\n",
        "from groq import Groq\n",
        "\n",
        "# Load environment variables (if using .env)\n",
        "load_dotenv()\n",
        "\n",
        "# Set API key (if not using .env)\n",
        "os.environ[\"GROQ_API_KEY\"] = \"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\"\n",
        "\n",
        "# Initialize Groq client\n",
        "client = Groq(api_key=os.getenv(\"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\"))\n",
        "\n",
        "# Now you can use the client\n",
        "chat_completion = client.chat.completions.create(\n",
        "    messages=[\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": \"Explain the importance of low latency LLMs\",\n",
        "        }\n",
        "    ],\n",
        "    model=\"llama3-8b-8192\",\n",
        ")\n",
        "print(chat_completion.choices[0].message.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g24WeNyzEe_p",
        "outputId": "2e3620d6-f972-4fa8-daca-c3e147c19ff4"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Large Language Models (LLMs) are artificial intelligence models that are trained on vast amounts of text data to generate human-like language understanding and generation capabilities. Low-latency LLMs are particularly important in applications where speed of analysis and response is crucial. Here are some reasons why:\n",
            "\n",
            "1. **Real-time analysis and feedback**: In applications like customer service chatbots, low-latency LLMs enable real-time analysis of user input and immediate response. This allows for a more seamless and efficient customer experience.\n",
            "2. **Uninterrupted conversation flow**: In applications like language translation or language-to-text summarization, low-latency LLMs ensure that the response is generated quickly, minimizing the delay between user input and system response. This preserves the natural flow of conversation.\n",
            "3. **Critical applications**: In critical applications like emergency response systems, medical diagnosis, or financial trading platforms, low-latency LLMs are essential to ensure timely and accurate decision-making.\n",
            "4. **Competitive advantage**: Businesses that integrate low-latency LLMs into their applications can gain a competitive advantage by processing user input and generating responses faster than their competitors.\n",
            "5. **Enhanced user engagement**: Low-latency LLMs can improve user engagement by providing instant responses, which can lead to increased user satisfaction and loyalty.\n",
            "6. **Reduced costs**: By reducing the time it takes to process and respond to user input, low-latency LLMs can help reduce costs associated with human customer support or manual analysis.\n",
            "7. **Increased accuracy**: Low-latency LLMs can also improve the accuracy of language processing tasks, as they can analyze and respond to user input in real-time, reducing the likelihood of errors.\n",
            "8. **Supporting emerging technologies**: Low-latency LLMs are essential for supporting emerging technologies like augmented reality, virtual reality, and autonomous vehicles, which require rapid processing and response times.\n",
            "9. **Improved accessibility**: Low-latency LLMs can enable real-time language translation and subtitling for individuals with hearing or speech impairments, promoting greater accessibility and inclusivity.\n",
            "10. **Advancements in research**: The development of low-latency LLMs can also drive advancements in natural language processing research, enabling researchers to explore new applications and use cases that benefit from the speed and accuracy of these models.\n",
            "\n",
            "In summary, low-latency LLMs are crucial for applications where speed and accuracy are paramount, enabling real-time analysis and response, competitive advantage, and enhanced user engagement, while also supporting emerging technologies and promoting accessibility.\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}